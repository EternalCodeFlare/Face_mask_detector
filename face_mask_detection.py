# -*- coding: utf-8 -*-
"""Face Mask Detection.ipynb..

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N1CN9s2VPAKFzQDJcQ4P11MKmeO_gCwe

# Import libraries and manage dataset
"""

from google.colab import files
files.upload()

# Code for importing dataset from Kaggle using the Kaggle API

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# Change permission
!chmod 600 ~/.kaggle/kaggle.json

# Download the dataset from Kaggle
!kaggle datasets download -d ashishjangra27/face-mask-12k-images-dataset



# Import all the required libraries 

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
import pathlib
import zipfile
import os
import PIL
import numpy as np
from keras.preprocessing import image_dataset_from_directory

from zipfile import ZipFile
file_name = "face-mask-12k-images-dataset.zip"

with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print("Done")

path = "/content/Face Mask Dataset"
print(path)

# Making train and validation directories

train_dir = os.path.join(path, 'Train')
validation_dir = os.path.join(path, 'Validation')

batch_size = 128
img_size = (160,160)

# Training images
train_dataset = image_dataset_from_directory(
    train_dir,
    shuffle = True,
    batch_size=batch_size,
    image_size = img_size
)

validation_dataset = image_dataset_from_directory(validation_dir,
                                                  shuffle=True,
                                                  batch_size=batch_size,
                                                  image_size=img_size)

class_names = train_dataset.class_names
print(class_names)

# Plot some images
plt.figure(figsize=(10,10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax= plt.subplot(3,3,i+1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    print(labels[i])
    plt.axis("off")



"""# Data Augmentation: Model with layers for Augmentation"""

data_augmentation = keras.Sequential([
        keras.layers.experimental.preprocessing.RandomFlip('horizontal',input_shape=(160,160,3)),
        keras.layers.experimental.preprocessing.RandomRotation(0.2),
        keras.layers.experimental.preprocessing.RandomZoom(0.2),
])

data_augmentation.summary()

"""# Visualizing the images"""

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10,10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3,3, i+1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0]/255)
    plt.axis('off')

# Preprocess the images when we need to feed them in the MobileNetV2

preprocess_input = keras.applications.mobilenet_v2.preprocess_input

val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)

"""# Develop base model from MobileNetV2"""

img_shape = img_size+(3,)
base_model = keras.applications.MobileNetV2(input_shape=img_shape,
                                            include_top = False,
                                            weights = 'imagenet')

image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)



"""# Feature Extraction
### Freeze the convolutional base
"""

# Currently we have to freeze all the layers
base_model.trainable = False

base_model.summary()

"""### In the above model the non-trainable parameters are full cause we have freezed all the layers"""

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(1,activation='sigmoid')
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)



"""## Add the classification layers above the convolutional base"""

inputs = tf.keras.Input(shape=(160, 160, 3))
x = data_augmentation(inputs)
x = preprocess_input(inputs)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.5)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)



"""## Compile the model"""

lr = 0.001
model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),
              loss = keras.losses.BinaryCrossentropy(from_logits=True),
              metrics = ['accuracy'])

model.summary()



"""## Train the initial model (not the final one)"""

initial_epochs = 5

loss0, accuracy0 = model.evaluate(validation_dataset)

print("initial loss: {:.2f}".format(loss0))
print("initial accuracy: {:.2f}".format(accuracy0))

history = model.fit(train_dataset,
                    epochs = initial_epochs,
                    validation_data = validation_dataset)

import matplotlib.pyplot as plt

def plot_graph(history, word):
  plt.plot(history.history[word])
  plt.plot(history.history['val_'+word])
  plt.xlabel('Epochs')
  plt.ylabel(word)
  plt.legend([word, 'val_'+ word])
  plt.show()

plot_graph(history, 'accuracy')
plot_graph(history, 'loss')



"""## Fine Tuning"""

base_model.trainable = True

print("Number of layers in base model: ", len(base_model.layers))

# Fine Tune from this layer onwards
fine_tune_at = 100

# Freeze all layers before the above mentioned layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False



"""## Compile the new model and Visulaize the model"""

model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = keras.optimizers.RMSprop(lr = lr/10),
              metrics = ['accuracy'])

keras.utils.plot_model(model, show_shapes=True)

model.summary()

"""## New Model training"""

fine_tune_epochs = 10
total_epochs = initial_epochs + fine_tune_epochs 

history_fine = model.fit(train_dataset,
                         epochs = total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data = validation_dataset)

import matplotlib.pyplot as plt

def plot_graph(history, word):
  plt.plot(history.history[word])
  plt.plot(history.history['val_'+word])
  plt.xlabel('Epochs')
  plt.ylabel(word)
  plt.legend([word, 'val_'+ word])
  plt.show()

plot_graph(history_fine, 'accuracy')
plot_graph(history_fine, 'loss')

"""## Testing"""

# 1 - Without a mask
# 0 - With mask
# Upload and check if the model classifies the image correctly
import numpy as np
from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()

for fn in uploaded.keys():
 
  # predicting images
  path = '/content/' + fn
  img = image.load_img(path, target_size=(160,160))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)
  
  print(classes[0])
  if classes[0]>0.5:
    plt.imshow(img)
    plt.axis('off')
    print("\n\nPerson is not wearing a mask\n")
  else:
    plt.imshow(img)
    plt.axis('off')
    print("\n\nPerson is wearing a mask\n" )





